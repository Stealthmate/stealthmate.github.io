---
layout: experience
title: Web Crawler の開発
toc: true
date: 2019-06-01
custom:
  period: 2019 年
  affiliation:
    title: 株式会社 LEMO
    link: '#hist-lemo'
  tags:
    - Python
    - scrapoxy
    - AWS
    - EC2
---

## 概要

株式会社 LEMO でアルバイトしていたとき、 web crawler の開発を任されたことがあります。何をクローリングするかについてはここでは触れませんが、大まかな要件としてとてつもなく大量にあるページをすべてたどる必要があるものでした。しかも、対象のページにある情報の構成はバラバラで、キレイに抽出するにはなかなかの工夫も必要でした。

このプロジェクトは最初から私が一人で請け負い、半年程進めました。残念ながら最終的にはコスパの観点から実現が難しいという結論になりましたが、それでも私にとっては良い勉強になりましたし、会社にとっても意味のある結果につながったのでここで少し触れておきます。

## 私の貢献

このプロジェクトの最終的な意義は、**対象のページをすべてクローリングするのにどれぐらいの時間と費用がかかるかを確かめること**でした。取りやめになったのもこの結果を把握しての決断でした。ゆえに、私の貢献もこの意義を果たしたことになります。

と、キレイに言っていますが、実際は結論に至るまでに色々とやりました。まずクローラーを作るのが私にとって初めてで、それらしき技術に慣れるのに少々時間がかかりました。参考までにこのプロジェクトでは Python の [scrapoxy](https://scrapoxy.io/) というライブラリと、何かの messaging queue を使いました[^1]。どちらも私にとっては初めてみるものでした。

それから、大量のページをクローリングするとなると分散処理も考えないといけないとなり、その考え方や調整の仕方を把握するのにもそれなりに時間がかかりました。

- 何 ms にリクエストを何本送れば良いのか？
- どれぐらいのリクエスト量が一番速く処理されるのか？
- どれぐらいのリクエストを送れば HTTP 429 が返ってくるのか？
- EC2 のどのインスタンスを使えば一番コスパが良いのか？
- などなど

これらの情報を集め、何回か実験を行い、結果をまとめてようやく結論に至りました。今思えば大学生活中にやった中でこれが一番おもしろかった実験かもしれません。

---
{: .footnote-separator}

[^1]: このプロジェクトを進めていたのは 2019 年頃ですが、本文を執筆している今 （2025 年）ではもう記憶が浅く、何かの MQ を使ってたという事実しか覚えていません・・・